# 机器翻译（MT）及相关技术

使用的是循环神经网络，但不是传统的循环神经网络

用神经网络解决这个问题通常称为**神经机器翻译**（NMT）

主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。

```
import os
os.listdir('/home/kesci/input/')

import sys
sys.path.append('/home/kesci/input/d2l9528/')
import collections
import d2l
import zipfile
from d2l.data.base import Vocab
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import data
from torch import optim

'''
数据预处理
将数据集清洗、转化为神经网络可以接受的的输入minbatch
'''
with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:
      raw_text = f.read()
print(raw_text[0:1000])

#数据清洗
def preprocess_raw(text):
    text = text.replace('\u202f', ' ').replace('\xa0', ' ')#处理乱码，那个空格
    out = ''
    for i, char in enumerate(text.lower()): #统一为小写，不然Go go会认为是两个单词
        if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':#在单词和标点之间加空格；如果char是标点，那么就加一个空格
            out += ' '
        out += char
    return out #完整的text

text = preprocess_raw(raw_text) #完整的text
print(text[0:1000])

#分词——每一个样本是一个字符串---变为单词组成的列表

num_examples = 50000
source, target = [], []
for i, line in enumerate(text.split('\n')):
    if i > num_examples:
        break
    parts = line.split('\t')#用'\t'就是tap键进行split成为三段
    if len(parts) >= 2:
        source.append(parts[0].split(' '))#前两项的第1项，用空格把每个单词分开
        target.append(parts[1].split(' '))#前两项的第2项，用空格把每个单词分开
        
source[0:3], target[0:3]
'''
统计句长
'''
d2l.set_figsize()
d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=['source', 'target'])
d2l.plt.legend(loc='upper right');

'''
建立词典
'''
def build_vocab(tokens):
    tokens = [token for line in tokens for token in line]#数据集中所有的单词连成列表
    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)

src_vocab = build_vocab(source)
len(src_vocab)#3789 英文单词列表中有3789个单词

'''
载入数据集
'''
def pad(line, max_len, padding_token):#使每一个batch输入的句子的长度相同，对句子进行pad
# line--- 句子   max_len----规定每个batch的句子最长的长度  padding_token----一种special token
    if len(line) > max_len:
        return line[:max_len] #大于句长就截断
    return line + [padding_token] * (max_len - len(line))
pad(src_vocab[source[0]], 10, src_vocab.pad)#[38, 4, 0, 0, 0, 0, 0, 0, 0, 0] 小于长度就补足，用0补足

def build_array(lines, vocab, max_len, is_source):
#               句子   单词表          看他是英语还是法语
    lines = [vocab[line] for line in lines] #vocab[line] 是前面Class Vocab中的__getitem__()，用中括号[]就可以调用，line就是单词列表，返回id列表；lines就是句子的id列表
    if not is_source:#不是source，那么就是target
        lines = [[vocab.bos] + line + [vocab.eos] for line in lines] #句子加上开始 和 结束 符号
    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])#；列表转tensor
    valid_len = (array != vocab.pad).sum(1) #valid_len，有效长度，保存这个句子原本的长度，计算loss只计算前面一部分，不是被pad的loss，第一个维度
    return array, valid_len
#数据生成器
def load_data_nmt(batch_size, max_len): # This function is saved in d2l.
    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)#build_vocab
    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)#source 英语
    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)#target 法语
    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)#生成训练数据，src_valid_len，英语有效长度；tgt_valid_len，法语有效长度
    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)
    return src_vocab, tgt_vocab, train_iter

src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)
for X, X_valid_len, Y, Y_valid_len, in train_iter:
    print('X =', X.type(torch.int32), '\nValid lengths for X =', X_valid_len,
        '\nY =', Y.type(torch.int32), '\nValid lengths for Y =', Y_valid_len)
    break

```
生成了两个英语句子，长度是8； batch_size=2，max_len=8
两个法语句子，长度是8
数据生成器，每次只生成一组，不会全部生成完在做一个循环
```
X = tensor([[ 28,  61,  10,  30, 106,   4,   0,   0],
        [150,  29,  11,   4,   0,   0,   0,   0]], dtype=torch.int32) 
Valid lengths for X = tensor([6, 4]) 
Y = tensor([[   1,   21,   90,  201,  151, 3252,    4,    2],
        [   1,   34,   12,   92,  160,   36,    4,    2]], dtype=torch.int32) 
Valid lengths for Y = tensor([8, 8])
```
- 1.数据集print(**raw_text**[0:1000])
输出的数据集：
>
- 法语到英语的翻译
- 每一行是一个例子，是一个样本例句
- 每一个样本的组成——
>> 
- Go.
英语，和后面的标点没有空格，需要将每个单词和标点进行区分
- tap键
- Va
法语
- 空格
这里的空格的编码是 '\xa0'，属于拉丁的扩展字符集，代表**不间断空白符**，超出了GBK的编码范围。所以，如果不处理的话，无法对其编码.
字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。
而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。
- !
- tap键
- 后面是不需要的了

```
Go.	Va !	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)
Hi.	Salut !	CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)
Hi.	Salut.	CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)
Run!	Cours !	CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)
Run!	Courez !	CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)
Who?	Qui ?	CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)
Wow!	Ça alors !	CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)
Fire!	Au feu !	CC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)
Help!	À l'aide !	CC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)
Jump.	Saute.	CC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)
Stop!	Ça suffit !	CC-BY 2.0 (France) Attribution: tato
```

- 2.print(text[0:1000]) 
清洗完成的text:
```
go .	va !	cc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev)
hi .	salut !	cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji)
hi .	salut .	cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux)
run !	cours !	cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic)
run !	courez !	cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic)
who?	qui ?	cc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux)
wow !	ça alors !	cc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)
fire !	au feu !	cc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic)
help !	à l'aide !	cc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko)
jump .	saute .	cc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)
stop !	ça suffit !	cc-b
```
- 3. 'source[0:3], target[0:3]'

source中每个元素是一个句子，句子的每个元素又是单词列表构成的
```
([['go', '.'], ['hi', '.'], ['hi', '.']],
 [['va', '!'], ['salut', '!'], ['salut', '.']])
```
- **d2l.data.base.Vocab**(tokens, min_freq=3, use_special_tokens=True)
建立词典
- 输入 tokens：所有单词组成的列表
- min_freq：最小词频，一个单词至少出现min_freq次才将它放进词典
- use_special_tokens：特殊字符，
>如开始符、结束符、padding、unknown
self.bos   self.eos self.pad self.unk；
为True，则四个特殊符都使用；为False，则只使用unk
![Image Name](https://cdn.kesci.com/upload/image/q5jc5ga5gy.png?imageView2/0/w/960/h/960)
- 1. 统计词频，存为元组 'counter = collections.Counter(tokens)';词频从大到小排序
- 形成的词典是,id word，相对应；其中特殊字符被赋为id为0,1，2,3,其他单词从后面开始
- 形成word到id 的映射；id到word的映射
>'_getitem(self,tokens)'
>>输入一个列表[A,B,C]，就会返回这三个单词对应的id，如[1,2,3]。

- class TensorDataste

![Image Name](https://cdn.kesci.com/upload/image/q5jc6e5tt1.png?imageView2/0/w/960/h/960)

- def pad(line, max_len, padding_token)
将每个batch中的句子长度变为一样，用0补足

- def build_array(lines, vocab, max_len, is_source)

return array, valid_len
把padding好的列表变为tensor


## Encoder-Decoder
可以应用在对话系统、生成式任务中
![Image Name](https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640)

- encoder：从输入到隐藏状态，将输入变成隐藏状态，将输入翻译为语义编码
经常是一个循环神经网络
- 语义编码：就是隐藏状态
- decoder：从隐藏状态到输出，将隐藏状态到输出，语义编码作为输入，用Decoder变为输出

机器翻译的难点就是：输入和输出长度不同

```
class Encoder(nn.Module): #将输入翻译为语义编码
    def __init__(self, **kwargs): #初始化
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args): #前向传播
        raise NotImplementedError

class Decoder(nn.Module):
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args): #语义编码c过来的初始状态
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError

class EncoderDecoder(nn.Module): #两个拼接
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args) #encoder的输出enc_outputs
        dec_state = self.decoder.init_state(enc_outputs, *args)#enc_outputs作为decoder的输入，就是init_state，初始化状态
        return self.decoder(dec_X, dec_state)#返回decoder的输出

```


### 一种encoder-decoder结构——Sequence to Sequence模型

### 模型：
- 训练  
![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)
句子按照时序输入（循环神经网络，e.g.gru, lstm）——>decoder（深层的语言模型，类似于周杰伦歌词的生成）

- 预测

![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)

1.输入<bos>，句子的开始符
2.根据encoder的输入（语义编码），和下面的输入<bos>，预测数bonjour
3.下一个输入是上一个预测的Bonjour，再输出一个新的

### 具体结构：
![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)

encoder是：n层的深层循环神经网络，recurrent layer
decoder是：n层的深层循环神经网络，recurrent layer

1.sources输入，英语句子的输入，是一个id的列表，如[1,1,3,4]
2.embedding层将每个单词翻译为向量，也就是词向量——可以是下载的训练好的词向量，也可以是当场训练的
#### Seq2SeqEncoder
```
class Seq2SeqEncoder(d2l.Encoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        self.num_hiddens=num_hiddens
        self.num_layers=num_layers
        self.embedding = nn.Embedding(vocab_size, embed_size)#Embedding层
        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)#循环神经网络——embed_size：每个输入的大小，每个单词的维度；num_hiddens：隐藏层的大小——就是隐藏层所含神经元的个数；num_layers：层数
   
    def begin_state(self, batch_size, device):#对encoder的初始化，这里是LSTM，需要对初始的隐藏层状态和初始的记忆细胞都初始化
        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),
                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]

    def forward(self, X, *args):#前向传播，输入是X
        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)——batch_size：几句话；seq_len：一句话有几个单词；embed_size：词向量的大小，几维的词向量
        X = X.transpose(0, 1)  # RNN needs first axes to be time
#循环神经网络的输入必须是时序的，时间是第0个维度；在自然语言中，就是句子的顺序是第0个维度，所以这里把embed_size和batch_size调换

        # state = self.begin_state(X.shape[1], device=X.device)
        out, state = self.rnn(X)

        #out :每个时序的输出（每个循环神经网络单元的输出）；state：生成语义编码的状态
        # The shape of out is (seq_len, batch_size, num_hiddens).
        # state contains the hidden state and the memory cell of the last time step, the shape is (num_layers, batch_size, num_hiddens)
        # state包含     隐藏层状态        和   记忆细胞状态

        return out, state


encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)
#单词表内10个单词；词向量维度是8维；隐藏层神经元的个数是16；循环神经网络的层数是2层
X = torch.zeros((4, 7),dtype=torch.long)#构造输入是，4句话（batch_size），每句话7个单词
output, state = encoder(X)
output.shape, len(state), state[0].shape, state[1].shape
```
```
(t/8/orch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))
```
- output:[7, 4, 16]，7：每个单词都会有一个输出；4:4句话；16就hidden_state的形状
- len(state)：2；包含了记忆细胞和隐藏状态两个部分
- state[0].shape：[2, 4, 16]
- state[1].shape：[2, 4, 16]
#### Seq2SeqDecoder

encoder的Ct和Ht作为语义编码，也是作为decoder的第一个隐藏单元的输入

- dense层：
```
class Seq2SeqDecoder(d2l.Decoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)

        #比上面的encoder的初始化多了dense层，将Ht的输出映射为单词的输出，输出的全连接层需要一个dense层，16个神经元映射到100个单词表里面
        self.dense = nn.Linear(num_hiddens,vocab_size)

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]#相当于是encode的输出 state，state包含 隐藏层状态        和 记忆细胞状态


    def forward(self, X, state):
        X = self.embedding(X).transpose(0, 1)#生成词向量
        out, state = self.rnn(X, state)#放入rnn，得到out 和 state；使用每一个out生成每一个时间步的单词
        # Make the batch to be the first dimension to simplify loss computation.
        out = self.dense(out).transpose(0, 1)#将颠倒过的向量再颠倒回来
        return out, state

decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)
state = decoder.init_state(encoder(X))
out, state = decoder(X, state)
out.shape, len(state), state[0].shape, state[1].shape
'''
(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))

10是vocab_size，单词表的大小，10个得分里选一个最高的得分作为当前句子的输出
2  记忆细胞和隐藏状态
torch.Size([2, 4, 16]) 分别是记忆细胞和隐藏状态的size
'''

#损失函数——之前给每个句子做了padding，计算损失函数时，加上的padding部分的损失应当是无效的，只计算有效长度的损失

#恢复为有效长度的样子
def SequenceMask(X, X_len,value=0):#X：一个batch的输入，理解为一个batch的损失;X_len:X的有效长度；填充可以使用 value=0 或者 value=-1
    maxlen = X.size(1)
    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]
    #torch.arange(maxlen)这个部分 放到 X_len同样的device上  ； 加 .to() 是放在同一个图上，容易计算
    X[~mask]=value #X[mask]长度X_len以后的部分全部用value来填充
    return X

X = torch.tensor([[1,2,3], [4,5,6]])#[1,2,3]是一句话，[4,5,6]是一句话
SequenceMask(X,torch.tensor([1,2]))#torch.tensor([1,2])：原本的有效长度是1和2
'''
tensor([[1, 0, 0],
        [4, 5, 0]])
'''
X = torch.ones((2,3, 4))
SequenceMask(X, torch.tensor([1,2]),value=-1) #用-1来填充
'''
tensor([[[ 1.,  1.,  1.,  1.],
         [-1., -1., -1., -1.],
         [-1., -1., -1., -1.]],

        [[ 1.,  1.,  1.,  1.],
         [ 1.,  1.,  1.,  1.],
         [-1., -1., -1., -1.]]])

'''
#损失函数——改写的交叉熵损失函数nn.CrossEntropyLoss;改写就是加上SequenceMask(weights, valid_length)这个部分
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    # pred shape: (batch_size, seq_len, vocab_size) 是上面decoder的out的输出（有几个句子，几个单词，单词表的size） pred就是batch_size*seq_len*每个单词的得分
    # label shape: (batch_size, seq_len) 不用记录单词的得分，只用记录得分最高的单词（记录每个句子，正确的单词的id）
    # valid_length shape: (batch_size, ) 每个句子的有效长度

    def forward(self, pred, label, valid_length): #使用forward生成一个weight
        # the sample weights shape should be (batch_size, seq_len)
'''
weights是label大小的全是1的tensor，每句话单词的位置都是用1表示（one-hot编码）
然后SequenceMask把非有效位置的1变为0,这样：1就表示有效位置，0代表padding位置
weight*output时就可以将padding部分的损失变为0，就只保留有效部分的损失
'''
        weights = torch.ones_like(label)
        weights = SequenceMask(weights, valid_length).float()
        self.reduction='none'
        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)#上一级 父类nn.CrossEntropyLoss 的forword
        return (output*weights).mean(dim=1) #output*weight再在第一维上取平均；第0维是batchsize；第1维是seq_len，就是每个单词

loss = MaskedSoftmaxCELoss()
loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))
# pred  ：torch.ones((3, 4, 10))  10是单词表，3是3个句子，4是句子里有4个单词，然后每个单词的位置都有十个单词对应的得分
# label ：torch.ones((3,4) 3个句子和4个正确的单词
# valid_length： torch.tensor([4,3,0])有效句子的长度，三个句子的长度分别是4，3,0
'''
tensor([2.3026, 1.7269, 0.0000])#最后一个为0是因为句子的有效长度为0
'''

#训练
def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l
    model.to(device)#model放在device里面
    optimizer = optim.Adam(model.parameters(), lr=lr)#优化器
    loss = MaskedSoftmaxCELoss()#损失函数
    tic = time.time()
    for epoch in range(1, num_epochs+1):
        l_sum, num_tokens_sum = 0.0, 0.0#l_sum：loss的总和；num_tokens_sum：单次数量的总和；用l_sum/num_tokens_sum计算出来的loss是比较有参考性的
        for batch in data_iter:#data_iter生成一个batch
            optimizer.zero_grad()#优化器梯度置0
            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]  #样本中的每个x，x包含了X（英语句子）、X_vlen（X的有效长度）、Y（法语句子）、Y_vlen（Y的有效长度），x要放入device里面
            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1  
'''
Y是包含了 bos、正式的句子需要的word、eos；
Y_input是decoder的输入，是不需要eos这个值的，所以decoder的输入只包含bos和word——Y[:,:-1];
Y_label是decoder生成的，不需要bos这个部分，因为生成的是从第一个单词开始到eos结束，所以只包含word好eos——Y[:,1:];
Y_vlen：Y的有效长度会减掉1，因为没有eos
'''
            
            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)#Y_hat是decoder模型预测出来的y值
            l = loss(Y_hat, Y_label, Y_vlen).sum()#计算Y_hat和Y_label的有效长度的交叉熵损失函数
            l.backward()#反向传播

            with torch.no_grad():#梯度裁剪
                d2l.grad_clipping_nn(model, 5, device)

            num_tokens = Y_vlen.sum().item()
            optimizer.step()
            l_sum += l.sum().item()
            num_tokens_sum += num_tokens

        if epoch % 50 == 0:#每50个epoch就输出计算出的平均loss——l_sum/num_tokens_sum
            print("epoch {0:4d},loss {1:.3f}, time {2:.1f} sec".format( 
                  epoch, (l_sum/num_tokens_sum), time.time()-tic))
            tic = time.time()

embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0
batch_size, num_examples, max_len = 64, 1e3, 10
lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()
src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(
    batch_size, max_len,num_examples)
encoder = Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model = d2l.EncoderDecoder(encoder, decoder)
train_ch7(model, train_iter, lr, num_epochs, ctx)

```
```
epoch   50,loss 0.101, time 36.2 sec
epoch  100,loss 0.047, time 36.0 sec
epoch  150,loss 0.032, time 37.1 sec
epoch  200,loss 0.028, time 37.0 sec
epoch  250,loss 0.028, time 36.9 sec
epoch  300,loss 0.024, time 38.6 sec
```
- 1. def train_ch7(model, data_iter, lr, num_epochs, device):
- **model** 是encode decode seq2seq的整个模型的结构
- **data_iter** 是数据生成器，之前写过
- **lr** 学习率
- **num_epochs**
- **device** 设备，cpu or gpu
如果是cpu，model.to(device)都可以忽略；
如果是gpu，参与计算的tensor和结构都得放在同一个设备中

- 2. device
这里放入device的有：
- 输入的tensor x，包含label——X, X_vlen, Y, Y_vlen；Y_input, Y_label, Y_vlen都是在device中，因为是x已经在device中生成的，所以都在device中
```
X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]
Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1  
```

### 测试
就是将上面的流程再走一遍
```
def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):
    src_tokens = src_vocab[src_sentence.lower().split(' ')] #小写化
    src_len = len(src_tokens) #这句话的有效长度
    if src_len < max_len: #对这句话进行padding
        src_tokens += [src_vocab.pad] * (max_len - src_len)
    enc_X = torch.tensor(src_tokens, device=device) #padding后的句子变为神经网络可以接受的输入，tensor，如若长度为10
    enc_valid_length = torch.tensor([src_len], device=device)
    # use expand_dim to add the batch_size dimension.
    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length) #enc_X.unsqueeze(dim=0)在前面加一个维度变为1*10了；enc_valid_length是有效长度
#enc_outputs是encoder的输出，也就是语义编码，也是下面model.decoder的初始化，也就是第一个隐藏层状态dec_state，还需要得到第一个输入dec_X ，也就是bos，神经元的输入

    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)
    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)
    predict_tokens = []
    for _ in range(max_len):
        Y, dec_state = model.decoder(dec_X, dec_state)#dec_X（bos）和dec_state（初始化的隐藏层状态）放入decoder，得到第一个循环神经单元的output Y和放入下一个神经单元的 dec_state（隐藏层状态）
        # The token with highest score is used as the next time step input.
        dec_X = Y.argmax(dim=2)#Y.argmax找到得分最高的单词（这个就是最优的输出），这个单词作为下一个循环神经单元的输入，
        py = dec_X.squeeze(dim=0).int().item()
        if py == tgt_vocab.eos:#如果这个单词等于eos，就是结束；否的话会继续循环，预测下一个单词
            break
        predict_tokens.append(py) #predict_tokens 预测
    return ' '.join(tgt_vocab.to_tokens(predict_tokens))

for sentence in ['Go .', 'Wow !', "I'm OK .", 'I won !']:
    print(sentence + ' => ' + translate_ch7(
        model, sentence, src_vocab, tgt_vocab, max_len, ctx))
```
```
Go . => va !
Wow ! => <unk> !
I'm OK . => je vais bien .
I won ! => j'ai gagné !
```
- 1.def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):
- **model** 模型
- **src_sentence** 一句话的输入，是字符串
- **src_vocab** 英语词典
- **tgt_vocab** 法语词典
- max_len
- device
>将src_sentence小写化——分词——

- 2.enc_X.**unsqueeze**(dim=0)
若enc_X是长度为 10 的tensor，那么经过**unsqueeze**(dim=0)就变成了1*10的tensor

## Beam Search
### 简单greedy search

![Image Name](https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440)

- 生成每一个时间步的单词：对于每一个时间步，都找到单词表里得分最高的单词，作为当前时间步的输出；这个输出再作为下一个时间步的输入；循环
- 但这个只考虑了当前的最优解，是**局部最优解**；没有考虑全局最优解

### 维特比算法：选择整体分数最高的句子（搜索空间太大） ，考虑**全局最优解**

### 集束搜索：结合 *简单greedy search* 和 *维特比算法*

![Image Name](https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640)
- **beem=2**

- 第一个单词找得分最高的两个（设定beem=2，就是集束的大小 = 2，就是找最好的两个）
- 再将这两个最优的和下一个单词组合，在所有组合中找最优的两个
- 再继续组合找最优的两个
- 直到找到 eos ，就结束。