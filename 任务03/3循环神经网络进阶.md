# 循环神经网络 ***RNN*** 进阶

GRU单元 门控循环网络
• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；  
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。  

## GRU模型从零实现
### 1.初始化参数
6个W 3个b
R（h大小）=   X*W ()（x.shape*h）    +H*W (h)(h*h)          + b(h的形状）

           num_inputs*num_hiddens  num_hiddens*num_hiddens   num_hiddens
Z H~ 的参数和R的相同

H-1 最开始的隐藏状态 初始化为0

### 输出 output
- 若为分类器：输出的H大小是h，分q个类别，则用Whq这个权重矩阵映射过去，bq的偏置 2个参数



```
import os
os.listdir('/home/kesci/input')

import numpy as np
import torch
from torch import nn, optim
import torch.nn.functional as F

import sys
sys.path.append("../input/")
import d2l_jay9460 as d2l
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()

'''
1.初始化参数
6个W 3个b
'''
num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size
print('will use', device)

def get_params():  
    def _one(shape): #权重的初始化
        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #随机初始化为正态分布  均值0 方差0.01
        return torch.nn.Parameter(ts, requires_grad=True) #数组转化为tensor
    def _three(): #打包起来 Wxh 和 Whh 最后偏置的初始化torch.zeros 全零的初始化
        return (_one((num_inputs, num_hiddens)),
                _one((num_hiddens, num_hiddens)),
                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))
     #9个参数的初始化
    W_xz, W_hz, b_z = _three()  # 更新门参数
    W_xr, W_hr, b_r = _three()  # 重置门参数
    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数
    
    # 输出层参数
    W_hq = _one((num_hiddens, num_outputs))
    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)

#get_params() 返回11个参数列表 9+2
    return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])

#隐藏状态的初始化 大小是batch_size*num_hiddens 全零初始化  batch_size里，每一个样本都需要一个H-1 将其初始化为0
def init_gru_state(batch_size, num_hiddens, device):   #隐藏状态初始化
    return (torch.zeros((batch_size, num_hiddens), device=device), )

'''
2.GRU模型 把公式写下来
'''
def gru(inputs, state, params):
#参数的舒适化
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)
        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)
        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = torch.matmul(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H,) #(H,) 最后一个隐藏状态
'''
3.训练模型
'''
#参数设置                                 衰减率
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']
#前缀

d2l.train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,
                          vocab_size, device, corpus_indices, idx_to_char,
                          char_to_idx, False, num_epochs, num_steps, lr,
                          clipping_theta, batch_size, pred_period, pred_len,
                          prefixes)

```

## GRU模型的简洁实现

不用自己写模型，不用初始化参数

- **nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)**

得到GRU layer

- **d2l.RNNModel(gru_layer, vocab_size).to(device)**

将GRU layer放入RNNModel

- **d2l.train_and_predict_rnn_pytorch**(**model**, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)

将model放入训练、预测

```
num_hiddens=256
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']

lr = 1e-2 # 注意调整学习率
gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)
model = d2l.RNNModel(gru_layer, vocab_size).to(device)
d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)

```
输出结果:

perplexity 困惑度
```
epoch 40, perplexity 1.026715, time 0.98 sec
 - 分开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧? 我的认真败
 - 不分开不 从前 喜欢 双截棍柔中带刚 想要去河南嵩山 学少林跟武当 快使用双截棍 哼哼哈兮 快使用双截棍 
epoch 80, perplexity 1.019797, time 1.00 sec
 - 分开的黑色幽默 想通 却又再考倒我 说散 你想很久了吧? 我不想拆穿你 当作 是你开的玩笑 想通 却又再
 - 不分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专
epoch 120, perplexity 1.013137, time 0.97 sec
 - 分开的黑色幽默 说散 你想很久了吧? 我的认真败给黑色幽默 走过了很多地方 我来到伊斯坦堡 就像是童话故
 - 不分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专
epoch 160, perplexity 1.019620, time 0.92 sec
 - 分开的黑色幽默 想通 却又再考倒我 说散 你想很久了吧? 我不想拆穿你 当作 是你开的玩笑 想通 却又再
 - 不分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专
```

## LSTM模型——长短期记忆 long short-term memory
- 记忆细胞：⼀种特殊的隐藏状态的信息的流动 控制输入有多少流入下一层
- 隐藏状态 
- 遗忘门：控制上一时间步的记忆细胞
- 输入门：控制当前时间步的输入
- 输出门：控制从记忆细胞到隐藏状态

### 初始化参数
- 输入： 8个W 4个b 12个

I =  X*W  + H *W  +b

h    x x*h h h*h   h 

- 输出： Whq  bq  2个参数

- 初始隐藏状态 H-1 1个
- 初始记忆细胞 C-1 1个
```
#初始化参数  12+2+2
num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size
print('will use', device)

def get_params():
    def _one(shape):
        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)
        return torch.nn.Parameter(ts, requires_grad=True)
    def _three():
        return (_one((num_inputs, num_hiddens)),
                _one((num_hiddens, num_hiddens)),
                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))

    #共12个
    W_xi, W_hi, b_i = _three()  # 输入门参数
    W_xf, W_hf, b_f = _three()  # 遗忘门参数
    W_xo, W_ho, b_o = _three()  # 输出门参数
    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数 
    
    # 输出层参数
    W_hq = _one((num_hiddens, num_outputs))
    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)
    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])

def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), 
            torch.zeros((batch_size, num_hiddens), device=device))

'''
定义LSTM模型	
'''
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)
        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)
        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)
        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * C.tanh()
        Y = torch.matmul(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H, C) #当前的H和C
'''
训练模型
'''
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']

d2l.train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,
                          vocab_size, device, corpus_indices, idx_to_char,
                          char_to_idx, False, num_epochs, num_steps, lr,
                          clipping_theta, batch_size, pred_period, pred_len,
                          prefixes)

```
输出结果：
```
epoch 40, perplexity 210.239498, time 1.57 sec
 - 分开 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我
 - 不分开 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我
epoch 80, perplexity 63.783046, time 1.66 sec
 - 分开 我想我这你的让我 想想你你的你 我想 你想你的我 我想 你想你的我 我想 你想你的我 我想 你想你
 - 不分开 我想你这想你的让 我 想你你的你 我想 你想你的我 我想 你想你的我 我想 你想你的我 我想 你想
epoch 120, perplexity 15.156336, time 1.52 sec
 - 分开 你在我的太笑 我想要你的你 快爱都没不球 静静悄悄 你使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮
 - 不分开 我已你这样你 一天一直不动 你爱你 一直我 我想就这样 你对了空 快给就空 你的没空 你的你空 你
epoch 160, perplexity 3.938926, time 1.53 sec
 - 分开 我想就你生微看每不想不能  我知道这里很美但 怎么我不爱不着  这样我说你要要 我要  穿我我想要
 - 不分开 我已经这生堡 每天歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害 你 靠着我的
```
## LSTM简洁实现
```
num_hiddens=256
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']

lr = 1e-2 # 注意调整学习率
lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)
model = d2l.RNNModel(lstm_layer, vocab_size)
d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)
```
输出结果：
```
epoch 40, perplexity 1.020419, time 1.01 sec
 - 分开 我有的话 你甘会听 不要再这样打我妈妈 难道你手不会痛吗 其实我回家就想要阻止一切 让家庭回到过去
 - 不分开 我的完美主义 太彻底 分手的话像语言暴力 我已无能为力再提起 决定中断熟悉 然后在这里 不限日期 
epoch 80, perplexity 1.013744, time 1.10 sec
 - 分开 我有的话 你甘会听 不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 我
 - 不分开 我将我的外婆家 一起看着日落 一直到我们都睡着 我想就这样牵着你的手不放开 爱可不可以简简单单没有
epoch 120, perplexity 1.010443, time 1.05 sec
 - 分开始了呼 她知道  杵在伊斯坦堡 却只想你和汉堡 我想要你的微笑每天都能看到  我知道这里很美但家乡的
 - 不分开 我的寂寞封闭 然后在这里 不限日期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场
epoch 160, perplexity 1.069448, time 1.04 sec
 - 分开 干什么 干什么 已被我一脚踢开 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 如果我有轻功 飞檐
 - 不分开 娘子她人在江南等我 泪不休 语沉默 娘子却依旧每日折一枝杨柳 在小村外的溪边河口 默默的在等著我 
```

## 深度循环神经网络

- 把一层的 output 作为下一层的 输入 input

- **nn.LSTM**(input_size=vocab_size, hidden_size=num_hiddens,**num_layers**=2)内置函数 

二层循环神经网络

num_layers——定义隐藏层的个数；默认是num_layers=1
```


num_hiddens=256
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']

lr = 1e-2 # 注意调整学习率
```
### 二层循环神经网络
```
#num_layers=2
gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=2)
model = d2l.RNNModel(gru_layer, vocab_size).to(device)
d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)
```
### 六层循环神经网络
```
#num_layers=6
gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=6)
model = d2l.RNNModel(gru_layer, vocab_size).to(device)
d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)

```
输出结果：
```
epoch 40, perplexity 282.703057, time 28.33 sec
 - 分开                                                  
 - 不分开                                                  
epoch 80, perplexity 281.562888, time 29.01 sec
 - 分开                                                  
 - 不分开
```
- 对比二层和六层发现二层比六层好；层数越多模型越复杂，对数据集的要求也更高，内容也更加抽象

## 双向循环神经网络
- 比普通的循环神经网络（H1到Ht的循环）多了一个从Ht到H1的循环，有两个方向的循环

- 将两个循环的层，拼接起来[H->,H<-]

- **nn.GRU**(input_size=vocab_size, hidden_size=num_hiddens,**bidirectional=True**)

加上一个参数bidirectional，就可以实现双向循环，默认为False，不采用双向循环

```
num_hiddens=128
num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e-2, 1e-2
pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']

lr = 1e-2 # 注意调整学习率

gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=True)
model = d2l.RNNModel(gru_layer, vocab_size).to(device)
d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,
                                corpus_indices, idx_to_char, char_to_idx,
                                num_epochs, num_steps, lr, clipping_theta,
                                batch_size, pred_period, pred_len, prefixes)

```
